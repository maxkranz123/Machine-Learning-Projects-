# Machine Learning Research Portfolio

This repository showcases comprehensive machine learning projects demonstrating expertise across fundamental and advanced ML concepts. Each notebook explores different aspects of machine learning, from traditional algorithms to state-of-the-art deep learning architectures.

## Project Overview

This portfolio contains 5 detailed Jupyter notebooks covering the complete spectrum of machine learning techniques:

### 1. Neural Networks & Traditional ML (`1_Neural_Networks.ipynb`)
**Focus:** Foundation of machine learning with multiple algorithm comparison

**Technologies & Concepts:**
- **Traditional ML Algorithms:** Logistic Regression, Random Forest, Gradient Boosting
- **Deep Learning:** Custom PyTorch Neural Network implementation
- **Dataset:** Scikit-learn digits dataset (handwritten digit recognition)
- **Model Evaluation:** Accuracy metrics, classification reports, confusion matrices
- **Model Deployment:** Streamlit web application for interactive predictions
- **Performance:** Achieved 97.8% accuracy with PyTorch neural network

**Key Skills Demonstrated:**
- Data preprocessing and feature scaling
- Cross-validation and train-test splitting
- Model comparison and evaluation
- PyTorch framework proficiency
- Web application development with Streamlit
- Model serialization and deployment

### 2. Convolutional Neural Networks (`2_CNNs.ipynb`)
**Focus:** Computer vision and advanced neural network architectures

**Technologies & Concepts:**
- **CNN Architecture:** Custom CNN implementation with Conv2D, MaxPooling, and fully connected layers
- **Transfer Learning:** VGG16 pre-trained model fine-tuning
- **Dataset:** FashionMNIST (clothing classification)
- **Architecture Visualization:** Network parameter calculation and visualization using torchsummary
- **Performance Optimization:** GPU utilization and batch processing
- **Results:** CNN achieved 91.68% accuracy vs MLP's 87.00%

**Key Skills Demonstrated:**
- Convolutional neural network design and implementation
- Transfer learning and model fine-tuning
- Network architecture analysis and parameter counting
- PyTorch advanced features (device management, data loaders)
- Computer vision preprocessing and data augmentation
- Performance comparison between different architectures

### 3. Natural Language Processing (`3_Natural_Language_Processing.ipynb`)
**Focus:** Text processing and sequence modeling

**Technologies & Concepts:**
- **Text Preprocessing:** Tokenization, stemming, lemmatization, stop word removal
- **Feature Engineering:** Bag of Words (BoW) and TF-IDF vectorization
- **Topic Modeling:** Latent Dirichlet Allocation (LDA) with Gensim
- **Sentiment Analysis:** Rule-based and neural network approaches
- **RNN Architectures:** Simple RNN, LSTM, and GRU implementations
- **Sequence Processing:** Variable-length text handling and padding

**Key Skills Demonstrated:**
- Natural language preprocessing pipelines
- Traditional NLP techniques (TF-IDF, LDA)
- Recurrent neural network architectures
- Sequence-to-sequence modeling
- Text classification and sentiment analysis
- PyTorch for NLP applications

### 4. HuggingFace & Transformers (`4_HuggingFace_GenerativeAI.ipynb`)
**Focus:** State-of-the-art transformer models and pre-trained architectures

**Technologies & Concepts:**
- **HuggingFace Ecosystem:** Transformers library, datasets, and model hub
- **Pre-trained Models:** BERT, RoBERTa, and other transformer architectures
- **Model Fine-tuning:** Custom dataset adaptation and training
- **Inference Pipelines:** Text classification, generation, and analysis
- **API Integration:** Google Generative AI and advanced language models
- **Multimodal Processing:** Text and image processing capabilities

**Key Skills Demonstrated:**
- HuggingFace Transformers library mastery
- Pre-trained model utilization and fine-tuning
- Advanced NLP with transformer architectures
- API integration and cloud-based ML services
- Model deployment and inference optimization
- Modern ML pipeline development

### 5. Large Language Models (`5_LLMs.ipynb`)
**Focus:** Advanced LLM usage and text generation techniques

**Technologies & Concepts:**
- **LLM Architecture:** Transformer-based language models (Phi-3)
- **Text Generation:** Greedy decoding, sampling strategies, and generation parameters
- **Performance Optimization:** Key-Value caching for efficient inference
- **Text Classification:** Multiple approaches using representation and generative models
- **Embedding Techniques:** Sentence transformers and similarity-based classification
- **Zero-shot Learning:** Classification without task-specific training data

**Key Skills Demonstrated:**
- Large language model implementation and optimization
- Advanced text generation and decoding strategies
- Embedding-based machine learning workflows
- Zero-shot and few-shot learning techniques
- Performance optimization and caching strategies
- Modern NLP evaluation methodologies

##  Technical Stack

**Programming Languages:** Python 3.11+

**Core ML Libraries:**
- PyTorch (Deep Learning)
- Scikit-learn (Traditional ML)
- NumPy & Pandas (Data Manipulation)
- Matplotlib & Seaborn (Visualization)

**NLP & AI Libraries:**
- HuggingFace Transformers
- NLTK & spaCy (Natural Language Processing)
- Sentence Transformers
- Google Generative AI

**Specialized Tools:**
- Streamlit (Web Applications)
- Google Colab (Cloud Computing)
- CUDA (GPU Acceleration)

##  Performance Highlights

- **Digit Recognition:** 97.8% accuracy with custom PyTorch neural network
- **Fashion Classification:** 91.68% accuracy with CNN vs 87.00% with MLP
- **Transfer Learning:** Successfully fine-tuned VGG16 for domain adaptation
- **Text Classification:** Multiple approaches achieving 80-85% accuracy on sentiment analysis
- **Model Optimization:** Demonstrated 5x speed improvement using KV caching

##  Key Competencies

**Machine Learning Fundamentals:**
- Supervised and unsupervised learning
- Model selection and hyperparameter tuning
- Cross-validation and performance evaluation
- Feature engineering and data preprocessing

**Deep Learning Expertise:**
- Neural network architecture design
- Convolutional neural networks for computer vision
- Recurrent neural networks for sequence modeling
- Transfer learning and fine-tuning techniques

**Advanced AI/ML:**
- Transformer architectures and attention mechanisms
- Large language model implementation
- Zero-shot and few-shot learning
- Model optimization and deployment

**Software Engineering:**
- Clean, documented, and modular code
- Version control and project organization
- Web application development
- API integration and cloud services

##  Getting Started

Each notebook is self-contained and includes:
- Detailed explanations of concepts and implementations
- Step-by-step code examples with comments
- Performance analysis and results visualization
- Best practices and optimization techniques

To run the notebooks:
1. Clone this repository
2. Install required dependencies (listed in each notebook)
3. Open notebooks in Jupyter or Google Colab
4. Follow the execution order for best results

##  Learning Outcomes

This portfolio demonstrates comprehensive understanding of:
- Machine learning algorithm selection and implementation
- Deep learning architecture design and optimization
- Natural language processing and text analysis
- Modern AI/ML frameworks and tools
- Model deployment and real-world applications


