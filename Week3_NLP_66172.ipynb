{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3bIvKtv9b8I"
      },
      "source": [
        "# Natural Language Processing Concepts and Demonstrations\n",
        "\n",
        "This Google Colab notebook provides explanations and runnable code snippets to demonstrate various Natural Language Processing (NLP) concepts discussed in the lecture.\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "This notebook will cover the following key NLP concepts:\n",
        "\n",
        "* Text Preprocessing\n",
        "* Feature Generation (Bag of Words and TF-IDF)\n",
        "* Topic Modeling (Latent Dirichlet Allocation)\n",
        "* Sentiment Analysis\n",
        "* Recurrent Neural Networks (RNNs, LSTMs, GRUs)\n",
        "\n",
        "## 2. Text Preprocessing\n",
        "\n",
        "Text preprocessing is a crucial step in NLP to transform raw text into an analyzable format. Common preprocessing techniques include:\n",
        "\n",
        "* **Lowercasing:** Converting all text to lowercase to ensure consistency.\n",
        "* **Cleaning and Normalization:** Removing HTML tags, accented characters, expanding contractions, standardizing spellings, removing extra whitespaces, punctuation, special characters, and converting number words to numeric form.\n",
        "* **Tokenization:** Splitting text into smaller units called tokens (e.g., words).\n",
        "* **Stemming:** Reducing inflected words to their root form (stem).\n",
        "* **Lemmatization:** Converting words to their base dictionary form (lemma).\n",
        "* **Stop Word Removal:** Removing common words that often have little semantic value (e.g., \"the\", \"a\", \"is\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HGAykfI9s9b",
        "outputId": "da06bce8-855a-4626-f628-a646c4ca4e5c"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TphDNGAg9WLB",
        "outputId": "7755f61d-f619-45e9-d4a3-8b834119d4c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lowercased: this is an example sentence with some common words like is and some inflected words like running and runs.\n",
            "Cleaned: this is an example sentence with some common words like is and some inflected words like running and runs\n",
            "Tokens: ['this', 'is', 'an', 'example', 'sentence', 'with', 'some', 'common', 'words', 'like', 'is', 'and', 'some', 'inflected', 'words', 'like', 'running', 'and', 'runs']\n",
            "Filtered tokens (stop words removed): ['example', 'sentence', 'common', 'words', 'like', 'inflected', 'words', 'like', 'running', 'runs']\n",
            "Stemmed tokens: ['exampl', 'sentenc', 'common', 'word', 'like', 'inflect', 'word', 'like', 'run', 'run']\n",
            "Lemmatized tokens: ['example', 'sentence', 'common', 'word', 'like', 'inflected', 'word', 'like', 'running', 'run']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "text = \"This is an example sentence with some common words like is and some inflected words like running and runs.\"\n",
        "\n",
        "# Lowercasing\n",
        "text_lower = text.lower()\n",
        "print(f\"Lowercased: {text_lower}\")\n",
        "\n",
        "# Cleaning (removing punctuation and extra spaces)\n",
        "text_cleaned = re.sub(r'[^\\w\\s]', '', text_lower)\n",
        "text_cleaned = \" \".join(text_cleaned.split())\n",
        "print(f\"Cleaned: {text_cleaned}\")\n",
        "\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(text_cleaned)\n",
        "print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# Stop word removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "print(f\"Filtered tokens (stop words removed): {filtered_tokens}\")\n",
        "\n",
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "stemmed_tokens = [porter.stem(w) for w in filtered_tokens]\n",
        "print(f\"Stemmed tokens: {stemmed_tokens}\")\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "print(f\"Lemmatized tokens: {lemmatized_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td-uMqDV9YP7"
      },
      "source": [
        "## 3. Feature Generation\n",
        "Feature generation involves transforming text data into numerical representations that machine learning models can understand. Two common techniques are:\n",
        "\n",
        "- Bag of Words (BoW): Represents each document as a collection of words, ignoring grammar and word order, and encodes documents as vectors based on the frequency of each word in a vocabulary.\n",
        "- TF-IDF (Term Frequency-Inverse Document Frequency): Weights words based on their frequency in a document and their inverse frequency across the entire corpus, giving higher importance to words that are frequent in a specific document but rare in general1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skPP68mB9cVf",
        "outputId": "04e473e9-cade-439d-e4d5-c77ff4bc100d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words Matrix:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "Vocabulary: {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "Vocabulary: {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_matrix = vectorizer_bow.fit_transform(documents)\n",
        "print(\"Bag of Words Matrix:\")\n",
        "print(bow_matrix.toarray())\n",
        "print(\"Vocabulary:\", vectorizer_bow.vocabulary_)\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(documents)\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"Vocabulary:\", vectorizer_tfidf.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9r17mSZ-Grj"
      },
      "source": [
        "## 4. Topic Modeling (Latent Dirichlet Allocation - LDA)\n",
        "Topic modeling is a technique to automatically discover the underlying topics in a collection of documents. Latent Dirichlet Allocation (LDA) is a popular probabilistic model that assumes each document is a mixture of topics and each topic is a mixture of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9LvNgw5-DfN",
        "outputId": "13ff657a-3c7a-40f0-ec01-939e8309e7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topics:\n",
            "(0, '0.106*\"god\" + 0.106*\"faith\" + 0.106*\"pray\" + 0.106*\"religion\" + 0.106*\"hope\" + 0.037*\"game\" + 0.036*\"sports\" + 0.036*\"team\" + 0.036*\"baseball\" + 0.036*\"recovery\"')\n",
            "(1, '0.086*\"injury\" + 0.086*\"medicine\" + 0.086*\"health\" + 0.086*\"recovery\" + 0.086*\"baseball\" + 0.086*\"team\" + 0.086*\"sports\" + 0.052*\"exercise\" + 0.052*\"news\" + 0.052*\"cardio\"')\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "documents = [\n",
        "    \"health medicine exercise cardio vascular cancer\",\n",
        "    \"sports news team baseball injury recovery\",\n",
        "    \"religion pray hope faith god\",\n",
        "    \"health injury medicine recovery\",\n",
        "    \"sports team baseball game\"\n",
        "]\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_documents = [doc.split() for doc in documents]\n",
        "\n",
        "# Create a dictionary (mapping from word to integer id)\n",
        "dictionary = corpora.Dictionary(tokenized_documents)\n",
        "\n",
        "# Convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_documents]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics\n",
        "print(\"Topics:\")\n",
        "for topic in lda_model.print_topics():\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLftC6RR-Tjx"
      },
      "source": [
        "## 5. Sentiment Analysis\n",
        "Sentiment analysis aims to identify and extract subjective information (opinions, emotions, attitudes) from text. A simple rule-based approach involves using sentiment lexicons, which are lists of words associated with positive or negative sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7JceHCp-JBO",
        "outputId": "4c257a89-cbf0-4ab1-9be2-a50606405285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review 1 Sentiment: Positive\n",
            "Review 2 Sentiment: Negative\n",
            "Review 3 Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "positive_words = [\"good\", \"best\", \"beautiful\", \"amazing\", \"fantastic\"]\n",
        "negative_words = [\"bad\", \"worst\", \"ugly\", \"awful\", \"poor\"]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "    for token in tokens:\n",
        "        if token in positive_words:\n",
        "            positive_count += 1\n",
        "        elif token in negative_words:\n",
        "            negative_count += 1\n",
        "\n",
        "    if positive_count > negative_count:\n",
        "        return \"Positive\"\n",
        "    elif negative_count > positive_count:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "review1 = \"This is a fantastic and amazing movie!\"\n",
        "review2 = \"The food was bad and the service was awful.\"\n",
        "review3 = \"The weather is cloudy today.\"\n",
        "\n",
        "print(f\"Review 1 Sentiment: {analyze_sentiment(review1)}\")\n",
        "print(f\"Review 2 Sentiment: {analyze_sentiment(review2)}\")\n",
        "print(f\"Review 3 Sentiment: {analyze_sentiment(review3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZPX1sNE-ZnP"
      },
      "source": [
        "## 6. Recurrent Neural Networks (RNNs, LSTMs, GRUs)\n",
        "Recurrent Neural Networks (RNNs) are designed to process sequential data by maintaining an internal state (memory). However, simple RNNs suffer from the vanishing gradient problem, limiting their ability to learn long-range dependencies. Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs) are extensions of RNNs that address this issue by introducing gating mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T6WgSUmv-Y7e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Embedding, RNN, LSTM, GRU, Linear\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Sample IMDB-like dataset (replace with actual IMDB loading if needed)\n",
        "reviews = [\n",
        "    \"This movie is fantastic and I loved it.\",\n",
        "    \"The acting was terrible and the plot was boring.\",\n",
        "    \"I really enjoyed this film, it was great.\",\n",
        "    \"Not a good movie, I would not recommend it.\",\n",
        "    \"Excellent performance by the cast.\",\n",
        "    \"Waste of time and money.\",\n",
        "    \"A must-see movie, highly recommended.\",\n",
        "    \"The story was confusing and the ending was bad.\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0] # 1 for positive, 0 for negative\n",
        "\n",
        "# 1. Data Preprocessing\n",
        "def preprocess_data(reviews, labels):\n",
        "    # Tokenization\n",
        "    tokens = [review.lower().split() for review in reviews]\n",
        "    # Vocabulary creation\n",
        "    all_tokens = [token for sublist in tokens for token in sublist]\n",
        "    vocabulary = Counter(all_tokens)\n",
        "    sorted_vocab = sorted(vocabulary, key=vocabulary.get, reverse=True)\n",
        "    word_to_index = {word: index + 2 for index, word in enumerate(sorted_vocab)}\n",
        "    word_to_index['<pad>'] = 0\n",
        "    word_to_index['<unk>'] = 1\n",
        "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
        "    # Convert tokens to indices and pad sequences\n",
        "    indexed_tokens = [[word_to_index.get(token, word_to_index['<unk>']) for token in review] for review in tokens]\n",
        "    max_length = max(len(seq) for seq in indexed_tokens)\n",
        "    padded_tokens = [seq + [word_to_index['<pad>']] * (max_length - len(seq)) for seq in indexed_tokens]\n",
        "    return np.array(padded_tokens), np.array(labels), word_to_index, index_to_word, max_length\n",
        "\n",
        "padded_tokens, labels, word_to_index, index_to_word, max_length = preprocess_data(reviews, labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_tokens, test_tokens, train_labels, test_labels = train_test_split(padded_tokens, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create PyTorch Datasets\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, tokens, labels):\n",
        "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokens[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = SentimentDataset(train_tokens, train_labels)\n",
        "test_dataset = SentimentDataset(test_tokens, test_labels)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpI8UNOF_ejp"
      },
      "source": [
        "### Data Preprocessing:\n",
        "\n",
        "A sample IMDB-like dataset of reviews and their corresponding sentiment labels (positive or negative) is created. In a real-world scenario, you would load the actual IMDB dataset using libraries like torchtext.\n",
        "The preprocess_data function performs the following steps:\n",
        "\n",
        "- Tokenization: Converts each review into a list of lowercase words.\n",
        "- Vocabulary Creation: Creates a vocabulary (mapping of unique words to indices) from all the tokens. <pad> is used for padding sequences to the same length, and <unk> is used for words not in the vocabulary.\n",
        "Token to Index Conversion: Converts each word in the reviews to its corresponding index in the vocabulary.\n",
        "- Padding: Pads shorter sequences with the <pad> index to make all sequences the same length, which is required for batch processing in RNNs.\n",
        "\n",
        "\n",
        "The preprocessed data is split into training and testing sets.\n",
        "A PyTorch Dataset class (SentimentDataset) is created to load the tokenized reviews and their labels.\n",
        "DataLoaders are created to handle batching and shuffling of the data during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOFuo6AFAkF8"
      },
      "source": [
        "### Model Architectures:\n",
        "\n",
        "- SimpleRNN:\n",
        "-- Embedding layer: Converts word indices into dense vector representations. vocab_size is the number of unique words in the vocabulary, and embedding_dim is the size of the embedding vectors.\n",
        "-- RNN layer: The core recurrent layer. embedding_dim is the input size, hidden_dim is the size of the hidden state vectors. nonlinearity='relu' specifies the activation function.\n",
        "-- Linear layer: A fully connected layer that maps the final hidden state to the output dimension (1 for binary sentiment classification).\n",
        "-- forward method: Defines the forward pass of the network. It passes the input through the embedding layer, then the RNN layer. The hidden state of the last time step is used as the representation of the entire sequence, which is then passed through the fully connected layer and a sigmoid activation function to get the probability of the sentiment being positive.\n",
        "\n",
        "- LSTM Model:\n",
        "Similar structure to SimpleRNN, but uses an LSTM layer instead of RNN. The LSTM layer internally manages cell state and hidden state to handle long-range dependencies.\n",
        "The forward method returns both the output and the hidden and cell states. We use the hidden state of the last time step.\n",
        "\n",
        "- GRUModel:\n",
        "Similar structure to SimpleRNN, but uses a GRU layer. GRU is a simplified version of LSTM with fewer parameters.\n",
        "The forward method returns both the output and the hidden state. We use the hidden state of the last time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsIQyjTI-VkV",
        "outputId": "e239fd47-d81a-48d5-80ab-d8730d70567e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Simple RNN...\n",
            "Evaluating Simple RNN...\n",
            "Test Loss: 3.6792, Accuracy: 0.5000\n",
            "\n",
            "Training LSTM...\n",
            "Evaluating LSTM...\n",
            "Test Loss: 4.0651, Accuracy: 0.5000\n",
            "\n",
            "Training GRU...\n",
            "Evaluating GRU...\n",
            "Test Loss: 4.0790, Accuracy: 0.5000\n"
          ]
        }
      ],
      "source": [
        "# 2. Define Model Architectures\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = RNN(embedding_dim, hidden_dim, batch_first=True) # batch_first=True for correct shape\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # hidden shape: (n_layers * n_directions, batch_size, hidden_dim)\n",
        "        # output shape: (seq_len, batch_size, hidden_dim)\n",
        "        # Take the output corresponding to the last non-padded token\n",
        "        # This assumes padding token is 0\n",
        "        #last_non_padded_idx = (text != 0).sum(dim=1) - 1\n",
        "        #last_hidden_states = hidden[:, range(text.shape[0]), last_non_padded_idx]\n",
        "        # Reshape the hidden state to match the expected input shape of the fully connected layer\n",
        "        last_hidden_states = hidden[-1]  # Get the last hidden state\n",
        "        #print(last_hidden_states.shape)\n",
        "        #return torch.sigmoid(self.fc(last_hidden_states)) # Use the hidden state of the last time step\n",
        "        return torch.sigmoid(self.fc(last_hidden_states)) # Use the hidden state of the last time step\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = LSTM(embedding_dim, hidden_dim, batch_first=True) # Added batch_first=True\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        # Pass the embedded input to the LSTM\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # Take the hidden state of the final timestep,\n",
        "        # and pass it through the fully connected layer\n",
        "        # output.shape = (batch_size, seq_len, hidden_size)\n",
        "        # hidden.shape = (num_layers * num_directions, batch_size, hidden_size)\n",
        "        #print(output.shape)\n",
        "        output = output[:, -1, :] # Get the output of the last time step\n",
        "        #print(output.shape)\n",
        "        return torch.sigmoid(self.fc(output)) # Return the prediction\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = GRU(embedding_dim, hidden_dim, batch_first=True) # Added batch_first=True\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        # Pass the embedded input to the GRU\n",
        "        output, hidden = self.gru(embedded)\n",
        "        # hidden.shape = (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # Take the hidden state of the final timestep,\n",
        "        # and pass it through the fully connected layer\n",
        "        # output.shape = (batch_size, seq_len, hidden_size)\n",
        "        output = hidden[-1] # Get the output of the last time step\n",
        "        #print(output.shape)\n",
        "        return torch.sigmoid(self.fc(output)) # Return the prediction\n",
        "\n",
        "# 3. Instantiate and Train Models\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "vocab_size = len(word_to_index)\n",
        "\n",
        "rnn_model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "gru_model = GRUModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "def train_model(model, data_loader, optimizer, criterion, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.round(output)\n",
        "            correct += (predictions == target).sum().item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct / len(data_loader.dataset)\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Training Simple RNN\n",
        "optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss()\n",
        "print(\"Training Simple RNN...\")\n",
        "train_model(rnn_model, train_loader, optimizer_rnn, criterion)\n",
        "print(\"Evaluating Simple RNN...\")\n",
        "evaluate_model(rnn_model, test_loader, criterion)\n",
        "\n",
        "# Training LSTM\n",
        "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "print(\"\\nTraining LSTM...\")\n",
        "train_model(lstm_model, train_loader, optimizer_lstm, criterion)\n",
        "print(\"Evaluating LSTM...\")\n",
        "evaluate_model(lstm_model, test_loader, criterion)\n",
        "\n",
        "# Training GRU\n",
        "optimizer_gru = torch.optim.Adam(gru_model.parameters(), lr=0.001)\n",
        "print(\"\\nTraining GRU...\")\n",
        "train_model(gru_model, train_loader, optimizer_gru, criterion)\n",
        "print(\"Evaluating GRU...\")\n",
        "evaluate_model(gru_model, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4HiWSfQ1-up",
        "outputId": "87065372-a8b7-46c1-9f77-4e83fece22ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "66172 is done!\n"
          ]
        }
      ],
      "source": [
        "print(\"66172 is done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
