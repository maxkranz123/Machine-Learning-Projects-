{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEuI2B5PB0Ef"
      },
      "source": [
        "## Sample Exam Questions\n",
        "\n",
        "Here are sample exam questions based on previous years:\n",
        "\n",
        "<br>\n",
        "\n",
        "**1. What is the primary purpose of an activation function in a neural network?**\n",
        "-    a) To initialize the weights of the network.\n",
        "-    b) To introduce non-linearity into the network.\n",
        "-    c) To calculate the loss function.\n",
        "-    d) To perform feature scaling.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) To introduce non-linearity into the network.</p>\n",
        "  <p><b>Explanation:</b> A neural network without a non-linear activation function is essentially a linear regression model, which is less powerful for complex tasks. Activation functions introduce non-linearity, enabling the network to learn and perform more complex tasks.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55QW5aq-Cjwp"
      },
      "source": [
        "**2. Which of the following is a common issue when training very deep neural networks?**\n",
        "-    a) Overfitting\n",
        "-    b) Vanishing or exploding gradients\n",
        "-    c) Saddle points\n",
        "-    d) All of the above\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> d) All of the above</p>\n",
        "  <p><b>Explanation:</b> Deep neural networks can suffer from overfitting due to a large number of parameters. Vanishing or exploding gradients can hinder training by making it difficult to update weights effectively. Saddle points can slow down learning by creating plateaus in the error surface.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAOcRv-TCquB"
      },
      "source": [
        "**3. What is the main advantage of using Convolutional Neural Networks (CNNs) for image recognition tasks compared to regular fully-connected neural networks?**\n",
        "-    a) CNNs require more parameters, making them more flexible.\n",
        "-    b) CNNs are better at handling tabular data.\n",
        "-    c) CNNs use parameter sharing and sparsity of connections, making them more scalable for large images.\n",
        "-    d) CNNs do not require activation functions.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) CNNs use parameter sharing and sparsity of connections, making them more scalable for large images.</p>\n",
        "  <p><b>Explanation:</b> Regular neural networks don't scale well to large images due to the huge number of parameters. CNNs address this through parameter sharing (using the same filter across different parts of the image) and sparsity of connections (neurons are only connected to a local region).</p>\n",
        "</details>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAE85O0kE-I3"
      },
      "source": [
        "**4. What is the role of a pooling layer in a CNN?**\n",
        "-    a) To increase the dimensionality of the feature maps.\n",
        "-    b) To introduce non-linearity into the network.\n",
        "-    c) To reduce the dimensionality of feature maps and control overfitting.\n",
        "-    d) To perform the initial convolution operation on the input image.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) To reduce the dimensionality of feature maps and control overfitting.</p>\n",
        "  <p><b>Explanation:</b> Pooling layers reduce the spatial dimensions of the input, which decreases the number of parameters and computations, thereby helping to control overfitting.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srr6GlgSE3z1"
      },
      "source": [
        "**5. In the context of Recurrent Neural Networks (RNNs), what is the \"vanishing gradient\" problem?**\n",
        "-    a) The model learns too quickly, leading to suboptimal solutions.\n",
        "-    b) The gradients become too large, causing unstable training.\n",
        "-    c) The gradients become very small as they are propagated back through time, making it difficult for earlier layers to learn.\n",
        "-    d) The network has too few layers to learn complex patterns.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) The gradients become very small as they are propagated back through time, making it difficult for earlier layers to learn.</p>\n",
        "  <p><b>Explanation:</b> During backpropagation through time in RNNs, the gradient can diminish exponentially (vanish), resulting in earlier layers learning very slowly or not at all. LSTMs were introduced to prevent this.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T1-xAkLGiwJ"
      },
      "source": [
        "**6. What is the primary purpose of word embeddings in Natural Language Processing (NLP)?**\n",
        "-    a) To represent words as sparse, high-dimensional vectors.\n",
        "-    b) To count the frequency of words in a document.\n",
        "-    c) To represent words as dense, lower-dimensional vectors that capture semantic meaning.\n",
        "-    d) To remove stop words from text.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) To represent words as dense, lower-dimensional vectors that capture semantic meaning.</p>\n",
        "  <p><b>Explanation:</b> Word embeddings aim to capture the meaning of words by representing them as dense vectors where semantically similar words are closer in the vector space.</p>\n",
        "</details>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55oZD3Q5Gpta"
      },
      "source": [
        "**7. What is the core idea behind the \"Attention is All You Need\" paper that introduced the Transformer architecture?**\n",
        "-    a) Replacing recurrent layers entirely with attention mechanisms.\n",
        "-    b) Using only convolutional layers for sequence processing.\n",
        "-    c) Increasing the depth of RNNs significantly.\n",
        "-    d) Relying solely on feed-forward neural networks.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> a) Replacing recurrent layers entirely with attention mechanisms.</p>\n",
        "  <p><b>Explanation:</b> The Transformer architecture, introduced in \"Attention is All You Need,\" is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely to process sequences.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RV3BSfIGwe4"
      },
      "source": [
        "**8. In the Transformer architecture, what is the purpose of \"multi-head attention\"?**\n",
        "-    a) To allow the model to jointly attend to information from different representation subspaces at different positions.\n",
        "-    b) To reduce the number of parameters in the model.\n",
        "-    c) To handle input sequences of varying lengths.\n",
        "-    d) To perform language translation into multiple languages simultaneously.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> a) To allow the model to jointly attend to information from different representation subspaces at different positions.</p>\n",
        "  <p><b>Explanation:</b> Multi-head attention allows the model to focus on different parts of the input sequence and learn different aspects of the relationships between words by using multiple attention \"heads\" in parallel, each projecting the input into a different representation subspace.</p>\n",
        "</details>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSFcol-MHB3A"
      },
      "source": [
        "**9. What is a key characteristic of BERT (Bidirectional Encoder Representations from Transformers)?**\n",
        "-    a) It is a decoder-only model.\n",
        "-    b) It processes text in a unidirectional manner (left-to-right).\n",
        "-    c) It uses a masked language model objective to learn bidirectional representations.\n",
        "-    d) It is primarily designed for image generation.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) It uses a masked language model objective to learn bidirectional representations.</p>\n",
        "  <p><b>Explanation:</b> BERT is pre-trained using a masked language model (MLM) task, where it predicts randomly masked words in a sentence by considering both their left and right context. This enables it to learn deep bidirectional representations.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOa-KvN_HH9n"
      },
      "source": [
        "**10. What is a \"Foundation Model\" in the context of modern AI?**\n",
        "-    a) A small model trained for a very specific task.\n",
        "-    b) A model trained on massive datasets at a broad scale that can be adapted to a wide range of downstream tasks.\n",
        "-    c) A model that does not require any training data.\n",
        "-    d) A model exclusively used for generating images.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) A model trained on massive datasets at a broad scale that can be adapted to a wide range of downstream tasks.</p>\n",
        "  <p><b>Explanation:</b> Foundation models are large-scale models pre-trained on vast amounts of data, enabling them to be adapted (e.g., fine-tuned) for various downstream tasks with less task-specific data.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9yjjh6qHZos"
      },
      "source": [
        "**11. What is \"Generative Adversarial Network\" (GAN)?**\n",
        "-   a) A single neural network that learns to generate data.\n",
        "-    b) A model that discriminates between real and fake data.\n",
        "-    c) A system of two neural networks, a generator and a discriminator, that compete with each other, with the generator learning to create plausible data and the discriminator learning to distinguish real from fake data.\n",
        "-    d) A type of reinforcement learning algorithm.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) A system of two neural networks, a generator and a discriminator, that compete with each other, with the generator learning to create plausible data and the discriminator learning to distinguish real from fake data.</p>\n",
        "  <p><b>Explanation:</b> GANs consist of two parts: a generator that learns to create plausible data, and a discriminator that learns to distinguish the generator's fake data from real data.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dB06gi2KgPK"
      },
      "source": [
        "**12. What is the role of the \"discriminator\" in a GAN?**\n",
        "-    a) To generate new data samples.\n",
        "-    b) To learn to distinguish the generator's fake data from real data.\n",
        "-    c) To provide the loss function for the generator.\n",
        "-    d) To select the best-generated samples.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) To learn to distinguish the generator's fake data from real data.</p>\n",
        "  <p><b>Explanation:</b> The discriminator in a GAN acts as a classifier, attempting to identify whether a given data instance is from the real dataset or generated by the GAN's generator.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj8gkTrQKgWT"
      },
      "source": [
        "**13. What is \"Stable Diffusion\"?**\n",
        "-    a) A type of recurrent neural network.\n",
        "-    b) A text-to-image model based on diffusion techniques.\n",
        "-    c) An algorithm for optimizing neural network training.\n",
        "-    d) A method for data augmentation.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) A text-to-image model based on diffusion techniques.</p>\n",
        "  <p><b>Explanation:</b> Stable Diffusion is a text-to-image model released in 2022 that uses diffusion techniques to generate images from text prompts.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9CT1h_GKgYx"
      },
      "source": [
        "**14. Which technique is used by ELMo (Embeddings from Language Models) to generate contextualized word embeddings?**\n",
        "-    a) Bag-of-Words\n",
        "-    b) A bidirectional LSTM trained on a language modeling task.\n",
        "-    c) Static word embeddings like Word2Vec.\n",
        "-    d) Transformer encoder blocks.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) A bidirectional LSTM trained on a language modeling task.</p>\n",
        "  <p><b>Explanation:</b> ELMo uses a bidirectional Long Short-Term Memory (LSTM) network, trained on a language modeling task, to create contextualized word embeddings by looking at the entire sentence.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0JeWet5Kga_"
      },
      "source": [
        "**15. What does the term \"transfer learning\" refer to in the context of deep learning models?**\n",
        "-    a) Training a model from scratch on a new dataset.\n",
        "-    b) Using a pre-trained model as a starting point for a new, related task.\n",
        "-    c) Combining multiple models to make predictions.\n",
        "-    d) A specific type of activation function.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) Using a pre-trained model as a starting point for a new, related task.</p>\n",
        "  <p><b>Explanation:</b> Transfer learning involves reusing a model developed for one task (often trained on a large dataset like ImageNet) as the starting point for a model on a second, related task. This can speed up training and improve performance, especially when the new task has limited data.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFCsQCXEKgdN"
      },
      "source": [
        "**16. What is the main difference between \"stemming\" and \"lemmatization\" in text preprocessing?**\n",
        "-    a) Stemming is faster but may not produce actual words, while lemmatization produces actual dictionary words but is slower.\n",
        "-    b) Lemmatization is always rule-based, while stemming uses a dictionary.\n",
        "-    c) Stemming converts words to uppercase, while lemmatization converts them to lowercase.\n",
        "-    d) Stemming is only used for English, while lemmatization can be used for any language.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> a) Stemming is faster but may not produce actual words, while lemmatization produces actual dictionary words but is slower.</p>\n",
        "  <p><b>Explanation:</b> Stemming typically uses algorithms to chop off word endings (affixes), which is fast but might result in non-existent words (stems). Lemmatization uses a vocabulary and morphological analysis to return the base or dictionary form of a word (lemma), making it more accurate but computationally more expensive as it may require a corpus.</p>\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "209RdX5lSTRm"
      },
      "source": [
        "**17. In the context of the Transformer architecture, what is \"positional encoding\" used for?**\n",
        "-    a) To represent the semantic meaning of words.\n",
        "-    b) To provide information about the order of tokens in the input sequence, as the self-attention mechanism itself does not inherently process order.\n",
        "-    c) To normalize the input embeddings.\n",
        "-    d) To reduce the dimensionality of the input vectors.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) To provide information about the order of tokens in the input sequence, as the self-attention mechanism itself does not inherently process order.</p>\n",
        "  <p><b>Explanation:</b> Since Transformers process all tokens in a sequence simultaneously through self-attention, they lack an inherent understanding of word order. Positional encodings are added to the input embeddings to inject information about the position of each token in the sequence.</p>\n",
        "</details>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ieySaHuSaKH"
      },
      "source": [
        "**18. What is a common technique to reduce overfitting in neural networks?**\n",
        "-    a) Using a very large learning rate.\n",
        "-    b) Training the model for an excessive number of epochs.\n",
        "-    c) Dropout, which randomly deactivates neurons during training.\n",
        "-    d) Increasing the complexity of the model architecture.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> c) Dropout, which randomly deactivates neurons during training.</p>\n",
        "  <p><b>Explanation:</b> Dropout is a regularization technique where, during training, some hidden units (neurons) are randomly omitted with a certain probability. This prevents neurons from co-adapting too much and helps the model generalize better to unseen data.</p>\n",
        "</details>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4UIvabhShU0"
      },
      "source": [
        "**19. What is the purpose of the \"softmax\" function, often used in the output layer of a classification neural network?**\n",
        "-    a) To calculate the error of the network.\n",
        "-    b) To convert a vector of scores (logits) into a probability distribution, where each element is between 0 and 1, and all elements sum to 1.\n",
        "-    c) To perform feature scaling on the input data.\n",
        "-    d) To initialize the weights of the output layer.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) To convert a vector of scores (logits) into a probability distribution, where each element is between 0 and 1, and all elements sum to 1.</p>\n",
        "  <p><b>Explanation:</b> The softmax function is used in multi-class classification to transform the raw output scores (logits) from the network into probabilities for each class. These probabilities are non-negative and sum to one.</p>\n",
        "</details>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r191Htc6TSbQ"
      },
      "source": [
        "**20. What is the \"Bag-of-Words\" (BoW) model in text representation?**\n",
        "-    a) A model that considers the grammatical structure and word order of a document.\n",
        "-    b) A model that represents a document as an unordered collection of its words, disregarding grammar and word order, and focuses on the frequency of each word.\n",
        "-    c) A model that uses neural networks to create dense vector representations of words.\n",
        "-    d) A model specifically designed for sentiment analysis.\n",
        "\n",
        "<details>\n",
        "  <summary>Answer and Explanation</summary>\n",
        "  <p><b>Answer:</b> b) A model that represents a document as an unordered collection of its words, disregarding grammar and word order, and focuses on the frequency of each word.</p>\n",
        "  <p><b>Explanation:</b> The Bag-of-Words approach treats each document as a collection of its individual words, ignoring grammar, word order, and sentence structure. It focuses on the occurrence or frequency of words within the document.</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nK-L0Q2TelD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPQx28eKU-Z0"
      },
      "source": [
        "# After you complete the exercise, print \"I have completed this exercise and I feel great!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vw4iUhhFVDuL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have completed this exercise and I feel great!\n"
          ]
        }
      ],
      "source": [
        "print(\"I have completed this exercise and I feel great!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
