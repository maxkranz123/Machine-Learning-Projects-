{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Convolutional Neural Networks (CNNs) with PyTorch\n",
        "\n",
        "\n",
        "Original file is located at\n",
        "    https://drive.google.com/file/d/1LESKvWyGptijIYwyOi0U3CZ1G3QvvYFy/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "uEN2ZLwIu5vs"
      },
      "id": "uEN2ZLwIu5vs"
    },
    {
      "cell_type": "markdown",
      "id": "de5963a2",
      "metadata": {
        "id": "de5963a2"
      },
      "source": [
        "\n",
        "## 1. CNNs, Visualization, Parameter Calculation, and Transfer Learning with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6188dec",
      "metadata": {
        "id": "f6188dec"
      },
      "source": [
        "**Goal:** This notebook extends the previous version by adding network architecture visualization and parameter calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36feef8",
      "metadata": {
        "id": "e36feef8"
      },
      "source": [
        "**Dataset:** FashionMNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47679f35",
      "metadata": {
        "id": "47679f35"
      },
      "source": [
        "**Outline:**\n",
        "1.  **Convolution Operation:** Understanding the core convolution operation.\n",
        "2.  **FashionMNIST Dataset:** Loading and visualizing the dataset.\n",
        "3.  **MLP Classifier:** Building, training, and **visualizing** a simple MLP.\n",
        "4.  **CNN Classifier:** Building, training, and **visualizing** a CNN.\n",
        "5.  **Parameter Calculation:** Detailed explanation and code for calculating the number of parameters.\n",
        "6.  **Transfer Learning with VGG16:** Using a pre-trained VGG16 model on ImageNet, freezing layers, fine-tuning, and evaluating performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f452290",
      "metadata": {
        "id": "2f452290"
      },
      "source": [
        "**Learning Objectives:**\n",
        "*   Understand the convolution operation and its parameters.\n",
        "*   Learn how to build CNN architectures using PyTorch.\n",
        "*   Compare the performance of MLPs and CNNs on image classification.\n",
        "*   **Visualize network architectures using `torchsummary`.**\n",
        "*   **Understand and calculate the number of parameters in neural networks.**\n",
        "*   Apply transfer learning using a pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3898f31",
      "metadata": {
        "id": "c3898f31"
      },
      "source": [
        "## 2. The Convolution Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a207c43",
      "metadata": {
        "id": "8a207c43"
      },
      "source": [
        "The convolution operation is the fundamental building block of CNNs. It involves sliding a small filter (or kernel) over an input image, performing element-wise multiplication, and summing the results to produce a single output value. This process is repeated for every possible position of the filter on the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6765aafb",
      "metadata": {
        "id": "6765aafb"
      },
      "source": [
        "**Key Parameters:**\n",
        "*   **Filter (Kernel):** A small matrix of weights that is convolved with the input image.\n",
        "*   **Stride:** The number of pixels by which the filter is shifted at each step.\n",
        "*   **Padding:** Adding extra pixels around the border of the input image to control the size of the output feature map.\n",
        "*   **Input Channels:** The number of channels in the input image (e.g., 1 for grayscale, 3 for RGB).\n",
        "*   **Output Channels:** The number of different filters used in the convolution layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d152a2",
      "metadata": {
        "id": "26d152a2"
      },
      "source": [
        "**Example:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda8e18f",
      "metadata": {
        "id": "cda8e18f"
      },
      "source": [
        "Let's create a simple example to illustrate the convolution operation in Python using NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6061267",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b6061267"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8905b45d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "8905b45d"
      },
      "outputs": [],
      "source": [
        "def convolution(image, kernel, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Performs a 2D convolution operation.\n",
        "\n",
        "    Args:\n",
        "        image: Input image (NumPy array).\n",
        "        kernel: Convolution filter (NumPy array).\n",
        "        stride: Stride value.\n",
        "        padding: Padding value.\n",
        "\n",
        "    Returns:\n",
        "        Output feature map (NumPy array).\n",
        "    \"\"\"\n",
        "    image_height, image_width = image.shape\n",
        "    kernel_height, kernel_width = kernel.shape\n",
        "\n",
        "    # Add padding\n",
        "    padded_image = np.pad(image, pad_width=padding, mode='constant')\n",
        "    padded_height, padded_width = padded_image.shape\n",
        "\n",
        "    # Calculate the output dimensions\n",
        "    output_height = (padded_height - kernel_height) // stride + 1\n",
        "    output_width = (padded_width - kernel_width) // stride + 1\n",
        "\n",
        "    # Create the output feature map\n",
        "    output = np.zeros((output_height, output_width))\n",
        "\n",
        "    # Perform the convolution\n",
        "    for i in range(output_height):\n",
        "        for j in range(output_width):\n",
        "            start_row = i * stride\n",
        "            start_col = j * stride\n",
        "            end_row = start_row + kernel_height\n",
        "            end_col = start_col + kernel_width\n",
        "            output[i, j] = np.sum(padded_image[start_row:end_row, start_col:end_col] * kernel)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b803f28a",
      "metadata": {
        "id": "b803f28a"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "input_image = np.array([[1, 2, 3],\n",
        "                        [4, 5, 6],\n",
        "                        [7, 8, 9]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6146d14",
      "metadata": {
        "id": "f6146d14"
      },
      "outputs": [],
      "source": [
        "convolution_filter = np.array([[1, 0, -1],\n",
        "                                [1, 0, -1],\n",
        "                                [1, 0, -1]]) # Vertical Edge Detection filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b00ae2",
      "metadata": {
        "id": "d0b00ae2"
      },
      "outputs": [],
      "source": [
        "output_feature_map = convolution(input_image, convolution_filter, stride=1, padding=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd083bfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd083bfb",
        "outputId": "ecca4202-26d0-4e66-895f-ed6d6f1d8b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Image:\n",
            " [[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "\n",
            "Convolution Filter:\n",
            " [[ 1  0 -1]\n",
            " [ 1  0 -1]\n",
            " [ 1  0 -1]]\n",
            "\n",
            "Output Feature Map:\n",
            " [[-6.]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Input Image:\\n\", input_image)\n",
        "print(\"\\nConvolution Filter:\\n\", convolution_filter)\n",
        "print(\"\\nOutput Feature Map:\\n\", output_feature_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a24c78",
      "metadata": {
        "id": "f7a24c78"
      },
      "source": [
        "\n",
        "## 3. Loading and Visualizing the FashionMNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e046b73",
      "metadata": {
        "id": "2e046b73"
      },
      "source": [
        "Now, let's load the FashionMNIST dataset using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28273764",
      "metadata": {
        "id": "28273764"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aed40d0",
      "metadata": {
        "id": "9aed40d0"
      },
      "outputs": [],
      "source": [
        "# Define the data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to [-1, 1]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac8c9b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ac8c9b3",
        "outputId": "1b6312eb-5acd-42fb-ab66-7cf8ed1ef5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.3MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 176kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.26MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the training and testing datasets\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2e2a20",
      "metadata": {
        "id": "1d2e2a20"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7484e30",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a7484e30"
      },
      "outputs": [],
      "source": [
        "# Define the class names\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c86860",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "68c86860"
      },
      "outputs": [],
      "source": [
        "# Function to display images\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # Convert from CxHxW to HxWxC\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d21fd7",
      "metadata": {
        "id": "b6d21fd7"
      },
      "outputs": [],
      "source": [
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72fa8d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "b72fa8d9",
        "outputId": "611b0f64-9e74-48d6-8011-f64578922ebc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trouser Ankle boot Ankle boot Bag  \n"
          ]
        }
      ],
      "source": [
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images[:4]))\n",
        "# Print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721802d7",
      "metadata": {
        "id": "721802d7"
      },
      "source": [
        "\n",
        "## 4. Building and Visualizing a Multilayer Perceptron (MLP) Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4837bf",
      "metadata": {
        "id": "bb4837bf"
      },
      "source": [
        "Let's build a simple Multilayer Perceptron (MLP) and visualize its architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1aeee4",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1d1aeee4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "try:\n",
        "  from torchsummary import summary  # Import torchsummary\n",
        "except:\n",
        "  !pip install torchsummary\n",
        "  from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f508191a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "f508191a"
      },
      "outputs": [],
      "source": [
        "# Define the MLP architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input: 28x28 flattened image\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)  # Output: 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1) # Flatten the image\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No softmax here, CrossEntropyLoss handles it\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c27d99cc",
      "metadata": {
        "id": "c27d99cc"
      },
      "outputs": [],
      "source": [
        "# Instantiate the MLP\n",
        "mlp_net = MLP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d210a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2d210a6",
        "outputId": "a897148d-adc2-4823-88ee-cbe53eecb833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 128]         100,480\n",
            "            Linear-2                   [-1, 64]           8,256\n",
            "            Linear-3                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 109,386\n",
            "Trainable params: 109,386\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.42\n",
            "Estimated Total Size (MB): 0.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print model summary (Visualizing the architecture)\n",
        "\n",
        "\n",
        "input_size = (1, 28, 28)  # Channel, Height, Width\n",
        "\n",
        "# Get the device of the model's parameters\n",
        "device = next(mlp_net.parameters()).device\n",
        "\n",
        "# Pass the device as a string ('cuda' or 'cpu')\n",
        "summary(mlp_net, input_size=input_size, device=device.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9f7a72",
      "metadata": {
        "id": "7a9f7a72"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer (Training code remains the same)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Understanding Cross-Entropy Loss and Softmax\n",
        "\n",
        "### 🧠 What is Softmax?\n",
        "\n",
        "The **Softmax function** is often used in classification models to convert raw output scores (logits) from a neural network into probabilities.\n",
        "\n",
        "For a vector of scores \\( z = [z_1, z_2, ..., z_n] \\), the Softmax output for class \\( i \\) is:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
        "$$\n",
        "\n",
        "This ensures:\n",
        "- All outputs are between 0 and 1\n",
        "- All outputs sum to 1 (valid probability distribution)\n",
        "\n",
        "---\n",
        "\n",
        "### 💥 What is Cross-Entropy Loss?\n",
        "\n",
        "**Cross-Entropy Loss** measures the difference between two probability distributions:\n",
        "- The true distribution (ground truth)\n",
        "- The predicted distribution (from Softmax)\n",
        "\n",
        "For a single example with true class \\( y \\) and predicted probabilities \\( p \\), the cross-entropy is:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\log(p_y)\n",
        "$$\n",
        "\n",
        "In general:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_{i=1}^{n} y_i \\log(p_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( y_i \\in \\{0, 1\\} \\) (one-hot encoded true labels)\n",
        "- \\( p_i \\) is the predicted probability for class \\( i \\)\n",
        "\n",
        "---\n",
        "\n",
        "### ❓ Are Softmax and Cross-Entropy the Same?\n",
        "\n",
        "**No**, but they are often **used together**:\n",
        "- **Softmax** converts logits to probabilities\n",
        "- **Cross-Entropy** calculates the loss based on those probabilities\n",
        "\n",
        "🧠 Most deep learning libraries (like PyTorch and TensorFlow) provide **`CrossEntropyLoss`** which:\n",
        "- Combines **Softmax + Cross-Entropy** into one efficient operation\n",
        "- Uses **log-softmax internally** for numerical stability\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary\n",
        "\n",
        "| Concept        | Purpose                                              |\n",
        "|----------------|------------------------------------------------------|\n",
        "| Softmax        | Converts raw scores to probabilities                 |\n",
        "| Cross-Entropy  | Measures how well predicted probabilities match labels |\n",
        "\n",
        "So while **they work hand-in-hand**, **Softmax is not the same as Cross-Entropy Loss**, but they’re often **used together** to train classification models.\n"
      ],
      "metadata": {
        "id": "8jDahTNudg1l"
      },
      "id": "8jDahTNudg1l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b60e3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07b60e3e",
        "outputId": "1fbe3d85-9c0e-4011-cce4-23672edc95ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.726\n",
            "[1,   400] loss: 0.502\n",
            "[1,   600] loss: 0.458\n",
            "[1,   800] loss: 0.434\n",
            "[2,   200] loss: 0.395\n",
            "[2,   400] loss: 0.384\n",
            "[2,   600] loss: 0.371\n",
            "[2,   800] loss: 0.378\n",
            "[3,   200] loss: 0.342\n",
            "[3,   400] loss: 0.341\n",
            "[3,   600] loss: 0.349\n",
            "[3,   800] loss: 0.334\n",
            "[4,   200] loss: 0.316\n",
            "[4,   400] loss: 0.324\n",
            "[4,   600] loss: 0.313\n",
            "[4,   800] loss: 0.316\n",
            "[5,   200] loss: 0.297\n",
            "[5,   400] loss: 0.298\n",
            "[5,   600] loss: 0.301\n",
            "[5,   800] loss: 0.297\n"
          ]
        }
      ],
      "source": [
        "# Training Loop (Unchanged)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "        outputs = mlp_net(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels) # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step() # Optimize\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 199: # Print every 200 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd3b70e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bd3b70e",
        "outputId": "0f74258c-5284-4fd7-83f1-f601a41d69d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training the MLP\n"
          ]
        }
      ],
      "source": [
        "print('Finished Training the MLP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e500835e",
      "metadata": {
        "id": "e500835e"
      },
      "outputs": [],
      "source": [
        "# Evaluate the MLP on the test set (Unchanged)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = mlp_net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc29129",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dc29129",
        "outputId": "54c87f8a-873d-4bcc-d2c4-ea3a272f1b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the MLP on the 10000 test images: 87.00%\n"
          ]
        }
      ],
      "source": [
        "print(f'Accuracy of the MLP on the 10000 test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10c49c87",
      "metadata": {
        "id": "10c49c87"
      },
      "source": [
        "\n",
        "## 5. Building and Visualizing a Convolutional Neural Network (CNN) Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54befac4",
      "metadata": {
        "id": "54befac4"
      },
      "source": [
        "Let's build a CNN and visualize its architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe60fde4",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fe60fde4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "try:\n",
        "  from torchsummary import summary  # Import torchsummary\n",
        "except:\n",
        "  !pip install torchsummary\n",
        "  from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9ec30e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ba9ec30e"
      },
      "outputs": [],
      "source": [
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # 1 input channel (grayscale), 32 output channels\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Input: 64 channels, 7x7 image size after pooling\n",
        "        self.fc2 = nn.Linear(128, 10) # Output: 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # Flatten for fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No softmax here, CrossEntropyLoss handles it\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e5800a",
      "metadata": {
        "id": "59e5800a"
      },
      "outputs": [],
      "source": [
        "# Instantiate the CNN\n",
        "cnn_net = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267d3f36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267d3f36",
        "outputId": "13291e03-4a1d-44ca-a292-f76891c6da87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
            "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
            "            Linear-5                  [-1, 128]         401,536\n",
            "            Linear-6                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 421,642\n",
            "Trainable params: 421,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.36\n",
            "Params size (MB): 1.61\n",
            "Estimated Total Size (MB): 1.97\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print model summary (Visualizing the architecture)\n",
        "input_size = (1, 28, 28)  # Channel, Height, Width\n",
        "summary(cnn_net, input_size=input_size, device=device.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c16d07",
      "metadata": {
        "id": "05c16d07"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer (Training code remains the same)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66e8d19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66e8d19",
        "outputId": "8088af7e-1a31-4f4f-a9ee-bb9e8f881041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.709\n",
            "[1,   400] loss: 0.428\n",
            "[1,   600] loss: 0.392\n",
            "[1,   800] loss: 0.344\n",
            "[2,   200] loss: 0.303\n",
            "[2,   400] loss: 0.279\n",
            "[2,   600] loss: 0.287\n",
            "[2,   800] loss: 0.274\n",
            "[3,   200] loss: 0.238\n",
            "[3,   400] loss: 0.251\n",
            "[3,   600] loss: 0.235\n",
            "[3,   800] loss: 0.233\n",
            "[4,   200] loss: 0.211\n",
            "[4,   400] loss: 0.211\n",
            "[4,   600] loss: 0.203\n",
            "[4,   800] loss: 0.197\n",
            "[5,   200] loss: 0.184\n",
            "[5,   400] loss: 0.181\n",
            "[5,   600] loss: 0.182\n",
            "[5,   800] loss: 0.185\n"
          ]
        }
      ],
      "source": [
        "# Training Loop (Unchanged)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "        outputs = cnn_net(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels) # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step() # Optimize\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 199: # Print every 200 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ff0178",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ff0178",
        "outputId": "0e33b7ab-1734-4577-8622-c9a35cc048e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training the CNN\n"
          ]
        }
      ],
      "source": [
        "print('Finished Training the CNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86a74f7",
      "metadata": {
        "id": "c86a74f7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the CNN on the test set (Unchanged)\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = cnn_net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db170087",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db170087",
        "outputId": "6352779f-c81c-4000-e565-4a394d99ad2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the CNN on the 10000 test images: 91.68%\n"
          ]
        }
      ],
      "source": [
        "print(f'Accuracy of the CNN on the 10000 test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b5ba202",
      "metadata": {
        "id": "3b5ba202"
      },
      "source": [
        "\n",
        "## 6. Parameter Calculation and Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72326aa",
      "metadata": {
        "id": "f72326aa"
      },
      "source": [
        "Let's break down how to calculate the number of parameters in each layer of our models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f7db27",
      "metadata": {
        "id": "d5f7db27"
      },
      "source": [
        "**A. Linear (Fully Connected) Layer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3e7e47",
      "metadata": {
        "id": "ce3e7e47"
      },
      "source": [
        "*   Formula: `(Input Features x Output Features) + Bias`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7f77ce",
      "metadata": {
        "id": "9c7f77ce"
      },
      "source": [
        "**Example (MLP Layer 1: `self.fc1 = nn.Linear(28 * 28, 128)`):**\n",
        "*   Input Features: 28 * 28 = 784\n",
        "*   Output Features: 128\n",
        "*   Parameters: (784 x 128) + 128 = 100480 + 128 = 100608 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38864fd",
      "metadata": {
        "id": "c38864fd"
      },
      "source": [
        "**B. Convolutional Layer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba56cd9",
      "metadata": {
        "id": "5ba56cd9"
      },
      "source": [
        "*   Formula: `(Kernel Height x Kernel Width x Input Channels x Output Channels) + Bias`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b187f049",
      "metadata": {
        "id": "b187f049"
      },
      "source": [
        "### **Example: `self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)`**\n",
        "\n",
        "- **Kernel Size**: 3 × 3  \n",
        "- **Input Channels**: 1 (grayscale image)  \n",
        "- **Output Channels**: 32  \n",
        "- **Parameters**: (3 × 3 × 1 × 32) + 32 = 288 + 32 = **320**\n",
        "\n",
        "---\n",
        "\n",
        "### **Example: `self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)`**\n",
        "\n",
        "- **Kernel Size**: 3 × 3  \n",
        "- **Input Channels**: 32  \n",
        "- **Output Channels**: 64  \n",
        "- **Parameters**: (3 × 3 × 32 × 64) + 64 = 18432 + 64 = **18,496**\n",
        "\n",
        "---\n",
        "\n",
        "### **After Flattening: `self.fc1 = nn.Linear(64*7*7, 128)`**\n",
        "\n",
        "- **Input Features**: 64 × 7 × 7 = 3136  \n",
        "- **Output Features**: 128  \n",
        "- **Parameters**: (3136 × 128) + 128 = 401408 + 128 = **401,536**\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Layer: `self.fc2 = nn.Linear(128, 10)`**\n",
        "\n",
        "- **Input Features**: 128  \n",
        "- **Output Features**: 10  \n",
        "- **Parameters**: (128 × 10) + 10 = 1280 + 10 = **1,290**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1bb1706",
      "metadata": {
        "id": "b1bb1706"
      },
      "source": [
        "**Important Notes:**\n",
        "*   Bias: Almost all layers have a bias term. The bias has one parameter for each output feature (or output channel in a convolutional layer).\n",
        "*   `torchsummary`: We use the `torchsummary` library to automatically calculate and display the number of parameters in each layer and the total number of parameters in the model.\n",
        "*   Understanding: Knowing how to calculate the number of parameters is important for understanding the complexity of your model and for debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25058f6a",
      "metadata": {
        "id": "25058f6a"
      },
      "source": [
        "**Code Example: Manual Parameter Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1b6cc9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1a1b6cc9"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the total number of trainable parameters in a PyTorch model.\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be13aa2",
      "metadata": {
        "id": "5be13aa2"
      },
      "outputs": [],
      "source": [
        "mlp_num_params = count_parameters(mlp_net)\n",
        "cnn_num_params = count_parameters(cnn_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc85414",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dc85414",
        "outputId": "79a7d2ae-6607-4cf3-ff29-7719a6c77bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters in MLP: 109386\n",
            "Total number of trainable parameters in CNN: 421642\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of trainable parameters in MLP: {mlp_num_params}\")\n",
        "print(f\"Total number of trainable parameters in CNN: {cnn_num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd89fb04",
      "metadata": {
        "id": "dd89fb04"
      },
      "source": [
        "## 7. Transfer Learning with VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "155e6dfb",
      "metadata": {
        "id": "155e6dfb"
      },
      "source": [
        "Now, let's apply transfer learning using a pre-trained VGG16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc64b53f",
      "metadata": {
        "id": "dc64b53f"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a049775f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a049775f",
        "outputId": "5f1dfdb0-5279-419c-b201-6137c46e5738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:08<00:00, 67.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the VGG16 model pre-trained on ImageNet\n",
        "vgg16 = models.vgg16(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8205e824",
      "metadata": {
        "id": "8205e824"
      },
      "outputs": [],
      "source": [
        "# Freeze all the layers of the VGG16 model\n",
        "for param in vgg16.parameters():\n",
        "    param.requires_grad = False # Prevents the weights from being updated during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114622d4",
      "metadata": {
        "id": "114622d4"
      },
      "outputs": [],
      "source": [
        "# Modify the classifier (fully connected layers) for our task\n",
        "# FashionMNIST has 10 classes, ImageNet has 1000\n",
        "num_features = vgg16.classifier[6].in_features # Number of input features to the last layer\n",
        "vgg16.classifier[6] = nn.Linear(num_features, 10) # Replace the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed996ede",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed996ede",
        "outputId": "65705b87-5b4f-4593-beea-8d117846de8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg16 = vgg16.to(device) # Move the whole model\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model summary (Visualizing the architecture)\n",
        "input_size = (3, 224, 224)  # Channel, Height, Width #Changed input size to (3, 224, 224)\n",
        "summary(vgg16, input_size=input_size, device=device.type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62LPjZ8I31lm",
        "outputId": "53fedc3a-05f6-4442-95c0-6fd8887f6471"
      },
      "id": "62LPjZ8I31lm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-4         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
            "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-7        [-1, 128, 112, 112]               0\n",
            "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
            "              ReLU-9        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-12          [-1, 256, 56, 56]               0\n",
            "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-14          [-1, 256, 56, 56]               0\n",
            "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-16          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
            "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-19          [-1, 512, 28, 28]               0\n",
            "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-21          [-1, 512, 28, 28]               0\n",
            "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
            "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-26          [-1, 512, 14, 14]               0\n",
            "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-28          [-1, 512, 14, 14]               0\n",
            "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-30          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "           Linear-33                 [-1, 4096]     102,764,544\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                 [-1, 4096]      16,781,312\n",
            "             ReLU-37                 [-1, 4096]               0\n",
            "          Dropout-38                 [-1, 4096]               0\n",
            "           Linear-39                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 134,301,514\n",
            "Trainable params: 7,120,394\n",
            "Non-trainable params: 127,181,120\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 218.77\n",
            "Params size (MB): 512.32\n",
            "Estimated Total Size (MB): 731.67\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d8dc80",
      "metadata": {
        "id": "a5d8dc80"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer (only train the classifier)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d53683c",
      "metadata": {
        "id": "7d53683c"
      },
      "outputs": [],
      "source": [
        "# Data transformation - VGG16 expects images of size 224x224\n",
        "transform_vgg = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resize images to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat grayscale 3 times to create 3 channels\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f4048c",
      "metadata": {
        "id": "37f4048c"
      },
      "outputs": [],
      "source": [
        "# Load FashionMNIST datasets with the new transformation\n",
        "trainset_vgg = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_vgg)\n",
        "testset_vgg = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_vgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a683263d",
      "metadata": {
        "id": "a683263d"
      },
      "outputs": [],
      "source": [
        "trainloader_vgg = torch.utils.data.DataLoader(trainset_vgg, batch_size=64, shuffle=True)\n",
        "testloader_vgg = torch.utils.data.DataLoader(testset_vgg, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8196ff69",
      "metadata": {
        "id": "8196ff69"
      },
      "outputs": [],
      "source": [
        "# Training Loop - Train *only* the classifier\n",
        "num_epochs = 10 # Train for fewer epochs because most layers are frozen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de663a8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de663a8e",
        "outputId": "1d77c6b6-899f-43df-a67b-d077b864e21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.702\n",
            "[1,   400] loss: 0.555\n",
            "[1,   600] loss: 0.533\n",
            "[1,   800] loss: 0.542\n",
            "[2,   200] loss: 0.530\n",
            "[2,   400] loss: 0.519\n",
            "[2,   600] loss: 0.525\n",
            "[2,   800] loss: 0.523\n",
            "[3,   200] loss: 0.512\n",
            "[3,   400] loss: 0.527\n",
            "[3,   600] loss: 0.517\n",
            "[3,   800] loss: 0.515\n",
            "[4,   200] loss: 0.517\n",
            "[4,   400] loss: 0.507\n",
            "[4,   600] loss: 0.522\n",
            "[4,   800] loss: 0.525\n",
            "[5,   200] loss: 0.527\n",
            "[5,   400] loss: 0.520\n",
            "[5,   600] loss: 0.518\n",
            "[5,   800] loss: 0.524\n",
            "[6,   200] loss: 0.517\n",
            "[6,   400] loss: 0.521\n",
            "[6,   600] loss: 0.502\n",
            "[6,   800] loss: 0.538\n",
            "[7,   200] loss: 0.518\n",
            "[7,   400] loss: 0.530\n",
            "[7,   600] loss: 0.500\n",
            "[7,   800] loss: 0.519\n",
            "[8,   200] loss: 0.513\n",
            "[8,   400] loss: 0.509\n",
            "[8,   600] loss: 0.526\n",
            "[8,   800] loss: 0.512\n",
            "[9,   200] loss: 0.502\n",
            "[9,   400] loss: 0.513\n",
            "[9,   600] loss: 0.513\n",
            "[9,   800] loss: 0.518\n",
            "[10,   200] loss: 0.507\n",
            "[10,   400] loss: 0.521\n",
            "[10,   600] loss: 0.508\n",
            "[10,   800] loss: 0.530\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader_vgg, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device) # Move data to the device\n",
        "\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "        outputs = vgg16(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels) # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step() # Optimize\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 199: # Print every 200 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93dff0aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93dff0aa",
        "outputId": "639b7d43-8314-4263-b389-3e4ef5cbf2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training the VGG16 Classifier\n"
          ]
        }
      ],
      "source": [
        "print('Finished Training the VGG16 Classifier')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67685e43",
      "metadata": {
        "id": "67685e43"
      },
      "outputs": [],
      "source": [
        "# Evaluate the VGG16 model on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # Disable gradient calculation\n",
        "    for data in testloader_vgg:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = vgg16(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fec9b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68fec9b7",
        "outputId": "a4579057-eb99-4f2c-db25-7c8b53867211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the VGG16 on the 10000 test images: 80.44%\n"
          ]
        }
      ],
      "source": [
        "print(f'Accuracy of the VGG16 on the 10000 test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90e5aaf",
      "metadata": {
        "id": "a90e5aaf"
      },
      "source": [
        "\n",
        "## 8. Fine-tuning the VGG16 model\n",
        "Let's try to fine tune the VGG16 to see if it improves the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd89bfdf",
      "metadata": {
        "id": "cd89bfdf"
      },
      "outputs": [],
      "source": [
        "# Unfreeze some of the last convolutional layers\n",
        "for param in vgg16.features[24:].parameters():  # Unfreeze last 2 conv layers\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3c3450",
      "metadata": {
        "id": "af3c3450"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer (train classifier and unfrozen conv layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vgg16.parameters()), lr=0.00001)  # Smaller learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc8eebe9",
      "metadata": {
        "id": "cc8eebe9"
      },
      "outputs": [],
      "source": [
        "# Training Loop - Train classifier and unfrozen conv layers\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb67face",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb67face",
        "outputId": "08a4a65f-692a-4b20-9904-3c8cecb919ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.443\n",
            "[1,   400] loss: 0.381\n",
            "[1,   600] loss: 0.362\n",
            "[1,   800] loss: 0.359\n",
            "[2,   200] loss: 0.309\n",
            "[2,   400] loss: 0.306\n",
            "[2,   600] loss: 0.305\n",
            "[2,   800] loss: 0.292\n",
            "[3,   200] loss: 0.275\n",
            "[3,   400] loss: 0.274\n",
            "[3,   600] loss: 0.263\n",
            "[3,   800] loss: 0.258\n",
            "[4,   200] loss: 0.247\n",
            "[4,   400] loss: 0.239\n",
            "[4,   600] loss: 0.232\n",
            "[4,   800] loss: 0.231\n",
            "[5,   200] loss: 0.221\n",
            "[5,   400] loss: 0.215\n",
            "[5,   600] loss: 0.211\n",
            "[5,   800] loss: 0.210\n",
            "[6,   200] loss: 0.199\n",
            "[6,   400] loss: 0.201\n",
            "[6,   600] loss: 0.200\n",
            "[6,   800] loss: 0.196\n",
            "[7,   200] loss: 0.183\n",
            "[7,   400] loss: 0.185\n",
            "[7,   600] loss: 0.177\n",
            "[7,   800] loss: 0.174\n",
            "[8,   200] loss: 0.166\n",
            "[8,   400] loss: 0.166\n",
            "[8,   600] loss: 0.161\n",
            "[8,   800] loss: 0.158\n",
            "[9,   200] loss: 0.150\n",
            "[9,   400] loss: 0.151\n",
            "[9,   600] loss: 0.147\n",
            "[9,   800] loss: 0.152\n",
            "[10,   200] loss: 0.135\n",
            "[10,   400] loss: 0.136\n",
            "[10,   600] loss: 0.138\n",
            "[10,   800] loss: 0.135\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader_vgg, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = vgg16(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 199:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef77a6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ef77a6c",
        "outputId": "3fd9de74-cb3f-4d5c-c51e-5c2490830201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Fine-tuning the VGG16\n"
          ]
        }
      ],
      "source": [
        "print('Finished Fine-tuning the VGG16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "551f659f",
      "metadata": {
        "id": "551f659f"
      },
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned VGG16 model on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader_vgg:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = vgg16(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c8aca5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68c8aca5",
        "outputId": "b2dd2e22-54bb-49ce-9c7f-b1f2fbb57d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Fine-tuned VGG16 on the 10000 test images: 91.70%\n"
          ]
        }
      ],
      "source": [
        "print(f'Accuracy of the Fine-tuned VGG16 on the 10000 test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After you finish all the code print out (\"I am done!\")\n"
      ],
      "metadata": {
        "id": "IjovN2VSz23Q"
      },
      "id": "IjovN2VSz23Q"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"I am done!\")"
      ],
      "metadata": {
        "id": "LRalQ8djz2C8",
        "outputId": "448af9eb-1589-40ab-bdb7-2c1a16e20795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LRalQ8djz2C8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a0aeaa8",
      "metadata": {
        "id": "6a0aeaa8"
      },
      "source": [
        "\n",
        "## 9. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "965d5108",
      "metadata": {
        "id": "965d5108"
      },
      "source": [
        "In this notebook, we:\n",
        "1.  Explained the convolution operation.\n",
        "2.  Loaded and visualized the FashionMNIST dataset.\n",
        "3.  Built a simple Multilayer Perceptron (MLP) classifier.\n",
        "4.  Built a Convolutional Neural Network (CNN) classifier and compared its performance to the MLP. The CNN achieved better performance due to its ability to learn spatial features.\n",
        "5.  **Visualized the network architectures of the MLP and CNN models using `torchsummary`.**\n",
        "6.  **Explained how to calculate the number of parameters in linear and convolutional layers.**\n",
        "7.  Applied transfer learning using a pre-trained VGG16 model on ImageNet, freezing layers and fine-tuning the classifier. Transfer learning often provides a performance boost and faster training, especially with limited data. We further improved the VGG16 model by unfreezing some of the convolutional layers.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58275335",
      "metadata": {
        "id": "58275335"
      },
      "source": [
        "**Key Takeaways:**\n",
        "*   CNNs are well-suited for image classification tasks due to their ability to learn spatial features.\n",
        "*   The `torchsummary` library is a useful tool for visualizing network architectures and counting parameters.\n",
        "*   Understanding parameter calculations is important for understanding model complexity and debugging.\n",
        "*   Transfer learning can significantly improve performance and reduce training time by leveraging pre-trained models.\n",
        "*   Fine-tuning pre-trained models can further enhance performance by adapting the model to the specific task."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PesZKZGuWXs"
      },
      "id": "8PesZKZGuWXs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}